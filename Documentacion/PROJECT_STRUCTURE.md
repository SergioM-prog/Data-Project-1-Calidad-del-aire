# Estructura del Proyecto - Data Project Calidad del Aire

DocumentaciÃ³n completa de la estructura de archivos y directorios del proyecto.

---

## Ãrbol de Directorios

```
Data-Project-1-Calidad-del-aire/
â”‚
â”œâ”€â”€ .git/                          # Control de versiones Git
â”œâ”€â”€ .gitignore                     # Archivos ignorados por Git
â”œâ”€â”€ .env                           # Variables de entorno (NO COMMITEAR)
â”œâ”€â”€ .env.example                   # Template de variables de entorno
â”‚
â”œâ”€â”€ docker-compose.yml             # OrquestaciÃ³n de contenedores
â”‚
â”œâ”€â”€ README.md                      # DocumentaciÃ³n principal
â”œâ”€â”€ ARCHITECTURE.md                # Arquitectura tÃ©cnica detallada
â”œâ”€â”€ PROJECT_STRUCTURE.md           # Este archivo
â”‚
â”œâ”€â”€ backend/                       # API Backend (FastAPI)
â”‚   â”œâ”€â”€ main.py                   # Endpoints y lÃ³gica de negocio
â”‚   â”œâ”€â”€ database.py               # InicializaciÃ³n de schemas y BD
â”‚   â”œâ”€â”€ config.py                 # ConfiguraciÃ³n de conexiÃ³n
â”‚   â”œâ”€â”€ requirements.txt          # Dependencias Python
â”‚   â””â”€â”€ Dockerfile                # Imagen Docker del backend
â”‚
â”œâ”€â”€ ingestion/                     # Servicio de ingesta de datos
â”‚   â”œâ”€â”€ main.py                   # Loop principal de ingesta
â”‚   â”œâ”€â”€ ciudades/                 # LÃ³gica especÃ­fica por ciudad
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ valencia.py           # Ingesta de Valencia API
â”‚   â”œâ”€â”€ requirements.txt          # Dependencias Python
â”‚   â””â”€â”€ Dockerfile                # Imagen Docker de ingestion
â”‚
â”œâ”€â”€ dbt/                           # Transformaciones dbt
â”‚   â””â”€â”€ air_quality_dbt/          # Proyecto dbt principal
â”‚       â”œâ”€â”€ dbt_project.yml       # ConfiguraciÃ³n del proyecto dbt
â”‚       â”œâ”€â”€ profiles.yml          # ConfiguraciÃ³n de conexiÃ³n a BD
â”‚       â”œâ”€â”€ packages.yml          # Paquetes dbt externos
â”‚       â”œâ”€â”€ models/               # Modelos SQL de transformaciÃ³n
â”‚       â”‚   â”œâ”€â”€ staging/          # Capa Silver - Limpieza
â”‚       â”‚   â”‚   â”œâ”€â”€ schema.yml
â”‚       â”‚   â”‚   â”œâ”€â”€ stg_valencia_air.sql
â”‚       â”‚   â”‚   â”œâ”€â”€ stg_valencia_air_historical_real_daily.sql
â”‚       â”‚   â”‚   â””â”€â”€ stg_valencia_air_historical_simulated_hourly.sql
â”‚       â”‚   â”‚
â”‚       â”‚   â”œâ”€â”€ intermediate/     # Capa Silver - Agregaciones
â”‚       â”‚   â”‚   â”œâ”€â”€ schema.yml
â”‚       â”‚   â”‚   â””â”€â”€ int_air_quality_union_hourly.sql
â”‚       â”‚   â”‚
â”‚       â”‚   â””â”€â”€ marts/            # Capa Gold - Fact Tables
â”‚       â”‚       â”œâ”€â”€ schema.yml
â”‚       â”‚       â”œâ”€â”€ fct_air_quality_hourly.sql
â”‚       â”‚       â”œâ”€â”€ fct_air_quality_daily.sql
â”‚       â”‚       â”œâ”€â”€ fct_alertas_actuales_contaminacion.sql
â”‚       â”‚       â”œâ”€â”€ fct_limites_de_contaminacion.sql
â”‚       â”‚       â”œâ”€â”€ fct_dim_estaciones.sql
â”‚       â”‚       â”œâ”€â”€ fct_calidad_aire_semanal.sql
â”‚       â”‚       â”œâ”€â”€ fct_ranking_estaciones_contaminadas.sql
â”‚       â”‚       â””â”€â”€ fct_air_quality_detailed_analysis.sql
â”‚       â”‚
â”‚       â”œâ”€â”€ seeds/                # Datos semilla (CSV estÃ¡ticos)
â”‚       â”œâ”€â”€ snapshots/            # Snapshots de datos histÃ³ricos
â”‚       â”œâ”€â”€ tests/                # Tests de calidad de datos
â”‚       â”œâ”€â”€ macros/               # Macros SQL reutilizables
â”‚       â””â”€â”€ dbt_packages/         # Paquetes instalados (git-ignored)
â”‚
â”œâ”€â”€ telegram_alerts/               # Sistema de alertas por Telegram
â”‚   â”œâ”€â”€ main.py                   # Loop de verificaciÃ³n de alertas
â”‚   â”œâ”€â”€ config.py                 # ConfiguraciÃ³n de Telegram Bot
â”‚   â”œâ”€â”€ requirements.txt          # Dependencias Python
â”‚   â””â”€â”€ Dockerfile                # Imagen Docker de alertas
â”‚
â”œâ”€â”€ grafana/                       # ConfiguraciÃ³n de Grafana
â”‚   â”œâ”€â”€ provisioning/             # ConfiguraciÃ³n automÃ¡tica
â”‚   â”‚   â””â”€â”€ datasources/          # Datasources configurados
â”‚   â”‚       â””â”€â”€ postgres.yml      # ConexiÃ³n a PostgreSQL
â”‚   â””â”€â”€ dashboards/               # Dashboards preconfigurados
â”‚       â””â”€â”€ air_quality.json      # Dashboard principal
â”‚
â”œâ”€â”€ frontend/                      # Dashboard Dash (opcional)
â”‚   â”œâ”€â”€ app.py                    # AplicaciÃ³n Dash/Plotly
â”‚   â”œâ”€â”€ requirements.txt          # Dependencias Python
â”‚   â””â”€â”€ Dockerfile                # Imagen Docker frontend
â”‚
â”œâ”€â”€ historical/                    # Datos histÃ³ricos (CSV)
â”‚   â”œâ”€â”€ real/                     # Datos reales histÃ³ricos
â”‚   â”‚   â””â”€â”€ *.csv                 # CSVs por aÃ±o/mes
â”‚   â””â”€â”€ simulated/                # Datos simulados para testing
â”‚       â””â”€â”€ *.csv                 # CSVs simulados
â”‚
â””â”€â”€ scripts/                       # Scripts utilitarios
    â”œâ”€â”€ generate_api_key.py       # Generador de API keys
    â””â”€â”€ setup_database.sh         # Script de inicializaciÃ³n (opcional)
```

---

## DescripciÃ³n Detallada de Archivos

### RaÃ­z del Proyecto

#### `docker-compose.yml`

**PropÃ³sito:** OrquestaciÃ³n de todos los servicios del proyecto.

**Servicios definidos:**
- `db` - PostgreSQL 17
- `backend` - FastAPI Barrier API
- `ingestion-valencia` - Servicio de ingesta
- `dbt` - Transformaciones SQL
- `telegram-alerts` - Sistema de alertas
- `grafana` - VisualizaciÃ³n
- `frontend` - Dashboard Dash (comentado)

**Configuraciones clave:**
```yaml
services:
  db:
    image: postgres:17-alpine
    ports: ["5431:5432"]
    volumes: [postgres_data:/var/lib/postgresql/data]

  backend:
    build: ./backend
    ports: ["8000:8000"]
    depends_on: [db]
    healthcheck: [curl, http://localhost:8000/health]
```

#### `.env`

**PropÃ³sito:** Variables de entorno sensibles (NO COMMITEAR).

**Variables requeridas:**
```env
# Database
POSTGRES_USER=postgres
POSTGRES_PASSWORD=<password>
POSTGRES_HOST=db
POSTGRES_PORT=5432
POSTGRES_DB=air_quality_db

# API Keys
INGESTION_VALENCIA_API_KEY=sk_xxx
TELEGRAM_ALERTS_API_KEY=sk_xxx
BARRIER_API_URL=http://backend:8000

# Telegram
BOT_TELEGRAM_TOKEN=123456:ABC...
ID_CANAL_TELEGRAM=-1001234567890

# Grafana
GF_SECURITY_ADMIN_PASSWORD=<password>
```

**âš ï¸ IMPORTANTE:** Este archivo DEBE estar en `.gitignore`.

#### `.gitignore`

**PropÃ³sito:** Archivos y directorios a ignorar por Git.

**Incluye:**
```
.env
__pycache__/
*.pyc
dbt/air_quality_dbt/dbt_packages/
dbt/air_quality_dbt/logs/
dbt/air_quality_dbt/target/
.DS_Store
```

---

### `/backend` - API Backend (FastAPI)

#### `backend/main.py` (500+ lÃ­neas)

**PropÃ³sito:** ImplementaciÃ³n del Barrier API Pattern.

**Responsabilidades:**
1. AutenticaciÃ³n M2M con API keys
2. Endpoints para ingesta de datos
3. Endpoints para consulta de alertas
4. Health checks

**Endpoints implementados:**

```python
@app.post("/api/ingest")
async def ingest_data(
    datos: list[AirQualityInbound],
    x_api_key: str = Depends(verify_api_key)
):
    # Recibe datos, valida, deduplica, inserta en raw

@app.get("/api/alertas")
async def get_alertas(
    x_api_key: str = Depends(verify_api_key)
):
    # Retorna alertas desde fct_alertas_actuales_contaminacion

@app.post("/api/alertas/registrar-envio")
async def registrar_alerta_enviada(
    alertas: list[AlertaEnviadaInbound],
    x_api_key: str = Depends(verify_api_key)
):
    # Registra alertas enviadas para evitar duplicados

@app.get("/health")
async def health_check():
    # Health check para Docker
```

**Modelos Pydantic:**
```python
class AirQualityInbound(BaseModel):
    objectid: int
    nombre: str
    fecha_carg: str
    so2: Optional[float] = None
    no2: Optional[float] = None
    # ... otros contaminantes

    model_config = ConfigDict(extra='forbid')
```

**LÃ­neas crÃ­ticas:**
- `93-114`: AutenticaciÃ³n M2M
- `150-185`: Ingesta con deduplicaciÃ³n
- `193-216`: Consulta de alertas

#### `backend/database.py` (400+ lÃ­neas)

**PropÃ³sito:** InicializaciÃ³n de schemas y carga de datos histÃ³ricos.

**Funciones principales:**

```python
def initialize_database():
    # Crea schemas: raw, staging, intermediate, marts, alerts, security

def create_raw_tables(engine):
    # Crea tablas raw para datos de Valencia

def create_security_tables(engine):
    # Crea tabla de API keys

def load_historical_data(engine):
    # Carga CSVs histÃ³ricos en raw tables
```

**Schemas creados:**
1. `raw` - Datos sin procesar
2. `staging` - (creado por dbt como views)
3. `intermediate` - (creado por dbt como tables)
4. `marts` - (creado por dbt como tables)
5. `alerts` - Historial de alertas
6. `security` - API keys

**LÃ­neas crÃ­ticas:**
- `281`: Riesgo de SQL injection (f-string)
- `303-398`: Carga de CSVs sin validaciÃ³n

#### `backend/config.py` (20 lÃ­neas)

**PropÃ³sito:** ConfiguraciÃ³n de conexiÃ³n a PostgreSQL.

```python
import os
from sqlalchemy import create_engine

DATABASE_URL = f"postgresql://{os.getenv('POSTGRES_USER')}:{os.getenv('POSTGRES_PASSWORD')}@{os.getenv('POSTGRES_HOST')}:{os.getenv('POSTGRES_PORT')}/{os.getenv('POSTGRES_DB')}"

engine = create_engine(DATABASE_URL, pool_pre_ping=True)
```

#### `backend/requirements.txt`

**Dependencias:**
```
fastapi
uvicorn[standard]
pandas
sqlalchemy
psycopg[binary]
pydantic
python-dotenv
```

---

### `/ingestion` - Servicio de Ingesta

#### `ingestion/main.py` (50 lÃ­neas)

**PropÃ³sito:** Loop principal de ingesta cada 30 minutos.

```python
INTERVAL_SECONDS = 1800  # 30 minutos

while True:
    try:
        datos = obtener_datos_valencia_api()
        enviar_a_backend(datos)
        print(f"Ingesta exitosa: {len(datos)} registros")
    except Exception as e:
        print(f"Error en ingesta: {e}")

    time.sleep(INTERVAL_SECONDS)
```

**LÃ­neas crÃ­ticas:**
- `17`: Intervalo hardcodeado (deberÃ­a ser env var)

#### `ingestion/ciudades/valencia.py` (100 lÃ­neas)

**PropÃ³sito:** LÃ³gica especÃ­fica para Valencia OpenDataSoft API.

**Funciones:**

```python
def obtener_datos_valencia_api():
    url = "https://valencia.opendatasoft.com/api/v2/catalog/datasets/..."
    response = requests.get(url, timeout=30)
    records = [record["fields"] for record in response.json()["records"]]
    return records

def enviar_a_backend(datos):
    validated = [AirQualityInbound(**record) for record in datos]
    response = requests.post(
        f"{BARRIER_API_URL}/api/ingest",
        headers={"X-API-Key": API_KEY},
        json=[d.model_dump() for d in validated]
    )
```

---

### `/dbt` - Transformaciones SQL

#### `dbt/air_quality_dbt/dbt_project.yml`

**PropÃ³sito:** ConfiguraciÃ³n del proyecto dbt.

```yaml
name: 'air_quality_dbt'
version: '1.0.0'
profile: 'air_quality_dbt'

models:
  air_quality_dbt:
    staging:
      +materialized: view
    intermediate:
      +materialized: table
    marts:
      +materialized: table
```

#### `dbt/air_quality_dbt/profiles.yml`

**PropÃ³sito:** ConfiguraciÃ³n de conexiÃ³n a PostgreSQL.

**âš ï¸ PROBLEMA:** Credenciales hardcodeadas (lÃ­neas 4-6).

```yaml
air_quality_dbt:
  target: dev
  outputs:
    dev:
      type: postgres
      user: postgres           # âŒ Hardcoded
      password: postgres       # âŒ Hardcoded
      dbname: air_quality_db   # âŒ Hardcoded
      host: db
      port: 5432
```

**SoluciÃ³n:**
```yaml
user: "{{ env_var('POSTGRES_USER') }}"
password: "{{ env_var('POSTGRES_PASSWORD') }}"
```

#### Modelos dbt

##### `models/staging/stg_valencia_air.sql`

**MaterializaciÃ³n:** View
**PropÃ³sito:** Limpieza y tipado de datos raw.

```sql
SELECT
    objectid,
    nombre,
    CAST(fecha_carg AS TIMESTAMP) AS fecha_hora,
    CAST(so2 AS FLOAT) AS so2,
    CAST(no2 AS FLOAT) AS no2,
    -- ... otros campos
FROM {{ source('raw', 'valencia_air_real_hourly') }}
WHERE fecha_carg IS NOT NULL
```

##### `models/intermediate/int_air_quality_union_hourly.sql`

**MaterializaciÃ³n:** Table
**PropÃ³sito:** UniÃ³n de real + histÃ³rico + simulado con deduplicaciÃ³n.

```sql
WITH real_data AS (
    SELECT * FROM {{ ref('stg_valencia_air') }}
),
historical_data AS (
    SELECT * FROM {{ ref('stg_valencia_air_historical_real_daily') }}
),
simulated_data AS (
    SELECT * FROM {{ ref('stg_valencia_air_historical_simulated_hourly') }}
),
unioned AS (
    SELECT * FROM real_data
    UNION ALL
    SELECT * FROM historical_data
    UNION ALL
    SELECT * FROM simulated_data
),
deduplicated AS (
    SELECT *,
           ROW_NUMBER() OVER (
               PARTITION BY objectid, fecha_hora
               ORDER BY fecha_carg DESC
           ) AS row_num
    FROM unioned
)
SELECT * FROM deduplicated WHERE row_num = 1
```

##### `models/marts/fct_alertas_actuales_contaminacion.sql`

**MaterializaciÃ³n:** Table
**PropÃ³sito:** Identificar alertas activas (valor > percentil 75).

```sql
WITH mediciones_recientes AS (
    SELECT * FROM {{ ref('fct_air_quality_hourly') }}
    WHERE fecha_hora >= NOW() - INTERVAL '2 hours'
),
limites AS (
    SELECT * FROM {{ ref('fct_limites_de_contaminacion') }}
)
SELECT
    m.objectid AS id_estacion,
    m.nombre AS nombre_estacion,
    m.fecha_hora AS fecha_hora_alerta,
    m.parametro,
    m.valor_medido,
    l.limite_p75,
    m.valor_medido - l.limite_p75 AS exceso
FROM mediciones_recientes m
JOIN limites l
    ON m.parametro = l.parametro
    AND EXTRACT(HOUR FROM m.fecha_hora) = l.hora_del_dia
WHERE m.valor_medido > l.limite_p75
```

---

### `/telegram_alerts` - Sistema de Alertas

#### `telegram_alerts/main.py` (150 lÃ­neas)

**PropÃ³sito:** Loop de verificaciÃ³n y envÃ­o de alertas cada 5 minutos.

**Proceso:**

```python
CHECK_INTERVAL = 300  # 5 minutos

while True:
    # 1. Obtener alertas desde backend
    alertas = requests.get(
        f"{BARRIER_API_URL}/api/alertas",
        headers={"X-API-Key": API_KEY}
    ).json()

    # 2. Para cada alerta
    for alerta in alertas:
        # 3. Formatear mensaje
        mensaje = formatear_mensaje_alerta(alerta)

        # 4. Enviar a Telegram
        enviar_mensaje_telegram(mensaje)

        # 5. Registrar como enviada
        registrar_alerta_enviada(alerta)

    time.sleep(CHECK_INTERVAL)
```

**Formato de mensaje:**

```python
def formatear_mensaje_alerta(alerta):
    return f"""
ğŸš¨ *ALERTA DE CONTAMINACIÃ“N* ğŸš¨

ğŸ“ *EstaciÃ³n:* {alerta['nombre_estacion']}
ğŸ•’ *Fecha/Hora:* {alerta['fecha_hora_alerta']}
ğŸ§ª *Contaminante:* {alerta['parametro']}
ğŸ“Š *Valor Medido:* {alerta['valor_medido']:.2f}
âš ï¸ *LÃ­mite P75:* {alerta['limite_p75']:.2f}
ğŸ“ˆ *Exceso:* {alerta['exceso']:.2f}
"""
```

**LÃ­neas crÃ­ticas:**
- `75-80`: Sin sanitizaciÃ³n de input (riesgo de inyecciÃ³n de markdown)

#### `telegram_alerts/config.py`

**PropÃ³sito:** ConfiguraciÃ³n del bot.

```python
import os

BOT_TOKEN = os.getenv("BOT_TELEGRAM_TOKEN")
CHANNEL_ID = os.getenv("ID_CANAL_TELEGRAM")
API_KEY = os.getenv("TELEGRAM_ALERTS_API_KEY")
BARRIER_API_URL = os.getenv("BARRIER_API_URL")
CHECK_INTERVAL = 300  # 5 minutos
```

---

### `/grafana` - VisualizaciÃ³n

#### `grafana/provisioning/datasources/postgres.yml`

**PropÃ³sito:** Auto-configuraciÃ³n del datasource PostgreSQL.

```yaml
apiVersion: 1

datasources:
  - name: PostgreSQL
    type: postgres
    access: proxy
    url: db:5432
    database: air_quality_db
    user: ${POSTGRES_USER}
    jsonData:
      sslmode: disable
      postgresVersion: 1700
    secureJsonData:
      password: ${POSTGRES_PASSWORD}
```

#### `grafana/dashboards/`

**PropÃ³sito:** Dashboards preconfigurados.

**Dashboards tÃ­picos:**
- Calidad del aire en tiempo real
- HistÃ³rico de contaminantes
- Alertas enviadas
- Comparativa de estaciones

---

### `/historical` - Datos HistÃ³ricos

#### `historical/real/`

**Contenido:** CSVs con datos reales histÃ³ricos (2014-2025).

**Formato:**
```csv
objectid,fecha,so2,no2,o3,co,pm10,pm25
1,2024-01-01,5.2,23.1,45.3,0.3,15.2,8.1
```

**Carga:** Se cargan automÃ¡ticamente en `raw.valencia_air_historical_real_daily` al iniciar el backend.

#### `historical/simulated/`

**Contenido:** CSVs con datos simulados (2025-2026) para testing.

**PropÃ³sito:** Backtesting y proyecciones.

---

### `/scripts` - Utilidades

#### `scripts/generate_api_key.py`

**PropÃ³sito:** Generar API keys para servicios M2M.

**Uso:**
```bash
python scripts/generate_api_key.py
```

**Output:**
```
Generando API keys para servicios...
INGESTION_VALENCIA_API_KEY=sk_1a2b3c4d5e6f...
TELEGRAM_ALERTS_API_KEY=sk_7g8h9i0j1k2l...

Agregar estas claves al archivo .env
```

---

## Flujo de Datos Entre Archivos

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FLUJO DE EJECUCIÃ“N                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. INICIO DEL SISTEMA
   docker-compose.yml
   â””â”€> Levanta servicios en orden:
       1. db (PostgreSQL)
       2. backend (espera db healthy)
       3. ingestion-valencia (espera backend healthy)
       4. dbt (loop cada 5 min)
       5. telegram-alerts (espera backend healthy)
       6. grafana

2. INGESTION LOOP
   ingestion/main.py
   â””â”€> ingestion/ciudades/valencia.py
       â””â”€> obtener_datos_valencia_api()
           â””â”€> POST backend/main.py::/api/ingest
               â””â”€> backend/database.py::raw tables

3. DBT TRANSFORMATIONS
   dbt run (cada 5 min)
   â””â”€> models/staging/*.sql (views)
       â””â”€> models/intermediate/*.sql (tables)
           â””â”€> models/marts/*.sql (fact tables)

4. ALERTAS
   telegram_alerts/main.py
   â””â”€> GET backend/main.py::/api/alertas
       â””â”€> Query marts.fct_alertas_actuales_contaminacion
           â””â”€> POST Telegram API
               â””â”€> POST backend/main.py::/api/alertas/registrar-envio
                   â””â”€> INSERT alerts.alertas_enviadas_telegram

5. VISUALIZACIÃ“N
   Grafana
   â””â”€> Query PostgreSQL:marts.*
       â””â”€> Render dashboards
```

---

## Archivos CrÃ­ticos (Orden de Importancia)

### Alta Criticidad

1. **`backend/main.py`** - Core API, sin esto nada funciona
2. **`docker-compose.yml`** - OrquestaciÃ³n, define toda la arquitectura
3. **`.env`** - Credenciales, sin esto servicios no arrancan
4. **`dbt/air_quality_dbt/models/marts/*.sql`** - Tablas finales de datos

### Media Criticidad

5. **`ingestion/ciudades/valencia.py`** - Ingesta de datos, pero puede fallar temporalmente
6. **`telegram_alerts/main.py`** - Alertas, pero no bloquea el sistema
7. **`backend/database.py`** - InicializaciÃ³n, se ejecuta una vez

### Baja Criticidad

8. **`grafana/dashboards/`** - VisualizaciÃ³n, se puede recrear
9. **`historical/`** - Datos histÃ³ricos, Ãºtiles pero no crÃ­ticos para operaciÃ³n
10. **`scripts/generate_api_key.py`** - Utilidad de setup

---

## TamaÃ±o de Archivos (Aproximado)

| Archivo/Directorio | TamaÃ±o | LÃ­neas de CÃ³digo |
|-------------------|--------|------------------|
| `backend/main.py` | 20 KB | ~500 LOC |
| `backend/database.py` | 15 KB | ~400 LOC |
| `dbt/models/` | 50 KB | ~1500 LOC SQL |
| `historical/` | 500 MB | N/A (CSV data) |
| `ingestion/` | 5 KB | ~150 LOC |
| `telegram_alerts/` | 6 KB | ~180 LOC |

---

## Convenciones de CÃ³digo

### Python

- **Formato:** PEP 8
- **Imports:** stdlib â†’ third-party â†’ local
- **Type hints:** Parcial (Pydantic models sÃ­, funciones no siempre)
- **Docstrings:** Limitados (mejorable)

### SQL (dbt)

- **Naming:** `snake_case`
- **Prefijos:** `stg_` (staging), `int_` (intermediate), `fct_` (fact)
- **CTEs:** Preferidos sobre subqueries
- **Formato:** IndentaciÃ³n 4 espacios

### Docker

- **Base images:** Alpine cuando sea posible
- **Multi-stage builds:** No usado (oportunidad de mejora)
- **Health checks:** Implementados en servicios crÃ­ticos

---

## Archivos que NO Existen (Pero DeberÃ­an)

### Tests

```
tests/
â”œâ”€â”€ test_backend_api.py
â”œâ”€â”€ test_ingestion.py
â”œâ”€â”€ test_pydantic_models.py
â””â”€â”€ conftest.py
```

### ConfiguraciÃ³n CI/CD

```
.github/
â””â”€â”€ workflows/
    â”œâ”€â”€ tests.yml
    â”œâ”€â”€ build.yml
    â””â”€â”€ deploy.yml
```

### DocumentaciÃ³n Adicional

```
docs/
â”œâ”€â”€ API.md              # DocumentaciÃ³n de endpoints
â”œâ”€â”€ DATA_DICTIONARY.md  # Diccionario de datos
â””â”€â”€ DEPLOYMENT.md       # GuÃ­a de deployment
```

---

## Referencias Cruzadas

- Ver [README.md](README.md) para guÃ­a de inicio rÃ¡pido
- Ver [ARCHITECTURE.md](ARCHITECTURE.md) para anÃ¡lisis tÃ©cnico profundo
- Ver `.gitignore` para archivos no rastreados

---

**Ãšltima actualizaciÃ³n:** 2026-01-29
**VersiÃ³n:** 1.0
